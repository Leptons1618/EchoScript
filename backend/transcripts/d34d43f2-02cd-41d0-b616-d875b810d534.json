{"text": " It's official, the AI hype train just went on life support, with the underwhelming release of GPT 4.5. Yesterday, OpenAI unveiled the most expensive AI model ever produced, yet it fails to crush any benchmarks, win any awards, or offer any novel capabilities whatsoever. Its only real selling point is vibes, and is supposed to chat in a more natural, human-like way. Don't get me wrong, it's a good model, but not good enough to feed the AI hype monster, and it looks increasingly likely that we're not headed into a technological singularity, but rather a sigmoid of sorrow. The Sam Altman couldn't even be bothered to leave his newborn kid in the hospital to show up to the product launch, and instead send in a bunch of interns to demo it. And that's crazy, because we're talking about Orion here. In 2023, tech leaders signed a petition to stop training big models like this. Altman himself begged the government to regulate it, and the only thing more disappointing than GPT 4.5 is the release of the Epstein files. In today's video, we'll find out if we just reached the limits of pre-training and generative pre-trained Transformers. It is February 28th, 2025, and you're watching the Code Report. I didn't want to make another crappy AI video today, but the bat signal was triggered. Any time an official video gets ratioed like this, I have no choice but to make a video. Before you unsubscribe, though, I've got an interesting Postgres video on the way. The first thing to know about GPT 4.5 is that it's extremely expensive. If you thought Claude was expensive at $15 per million tokens, GPT 4.5 is five times more expensive at $75 per million output tokens. Actually, no, correction, that's input tokens. It's $150 per million output tokens. And to chat with it, it's currently only available to the $200 per month pro users. I tried it out myself, and it does seem to emit chill vibes, but the problem is that's highly subjective. However, in the launch, OpenAI talked about a new vibes benchmark that's supposed to measure creative thinking. The best way to get a feel for the model is to talk to it, so let's jump into a demo. A lot of people on the internet criticized this presentation, but as an introvert myself, I think they did a great job. In addition, it apparently has a far lower hallucination rate, but what I found is that it still makes a lot of silly mistakes. It's not self-aware and has no idea what GPT 4.5 even is, and says its training cutoff is October 2023. It was, however, able to tell me how many Rs are in strawberry. That felt like a huge leap forward, but I quickly became disappointed when it gave me the wrong number of Ls in Lollapalooza. Now, when it comes to programming and science, I didn't even try, because we already know it's not going to perform as well as the deep thinking models like 03. Then to make matters worse, on the AIDAR polyglot coding benchmark, it's not only worse at programming than DeepSeek, but also hundreds of times more expensive. Now, if you're an Elon Musk hater, you'll want to take a bong rip of Copium right now, because currently XAI's Grok is the best model in the world. That's not my opinion, it's the opinion of the betting market. Although by the end of 2025, OpenAI is still the favorite to have the best model, but its odds are on the decline. That's problematic for OpenAI though, because they're raising billions and billions of dollars as they transition to for-profit and will need to maintain a massive valuation. Altman says there is no wall and believes they can scale these models almost infinitely. That's assuming he gets trillions of dollars from SoftBank and the Saudis to build these data centers. My theory as an unqualified shipposter is that they failed to train GPT 5 with any significant improvement, despite scaling up the number of parameters and compute. GPT 4.5 is the biggest model they've ever created, and now they're lowering the bar for GPT 5, which Altman described a few weeks ago being more like a router that automatically chooses the best model based on your prompt. And that's highly disappointing, because I was expecting to be a post-apocalyptic warlord by now, battling robots and barbecuing rats over burning garbage cans for dinner. But instead I live in this dystopia where artificial superintelligence never comes and nothing ever happens. But if you're a computer science student, the plateau is great news. AI coding tools are incredible, but they're most useful to real human programmers who know what they're doing. And I don't see that changing anytime soon. And you can start getting really good at programming for free thanks to this video's sponsor, Brilliant. Their platform provides interactive hands-on lessons that demystify the complexity of deep learning. With just a few minutes of effort each day, you can understand the math and computer science behind this seemingly magic technology. I'd recommend starting with Python, then check out their full How Large Language Models Work course if you really want to look under the hood of chat GPT. Try everything Brilliant has to offer for free for 30 days by going to brilliant.org slash fireship or use the QR code on screen. This has been the Code Report. Thanks for watching, and I will see you in the next one.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.96, "text": " It's official, the AI hype train just went on life support, with the underwhelming release of GPT 4.5.", "tokens": [50364, 467, 311, 4783, 11, 264, 7318, 24144, 3847, 445, 1437, 322, 993, 1406, 11, 365, 264, 833, 8746, 2810, 4374, 295, 26039, 51, 1017, 13, 20, 13, 50662], "temperature": 0.0, "avg_logprob": -0.133174920693422, "compression_ratio": 1.531986531986532, "no_speech_prob": 0.0007792838150635362}, {"id": 1, "seek": 0, "start": 5.96, "end": 9.88, "text": " Yesterday, OpenAI unveiled the most expensive AI model ever produced,", "tokens": [50662, 19765, 11, 7238, 48698, 47430, 264, 881, 5124, 7318, 2316, 1562, 7126, 11, 50858], "temperature": 0.0, "avg_logprob": -0.133174920693422, "compression_ratio": 1.531986531986532, "no_speech_prob": 0.0007792838150635362}, {"id": 2, "seek": 0, "start": 9.88, "end": 15.44, "text": " yet it fails to crush any benchmarks, win any awards, or offer any novel capabilities whatsoever.", "tokens": [50858, 1939, 309, 18199, 281, 10321, 604, 43751, 11, 1942, 604, 15193, 11, 420, 2626, 604, 7613, 10862, 17076, 13, 51136], "temperature": 0.0, "avg_logprob": -0.133174920693422, "compression_ratio": 1.531986531986532, "no_speech_prob": 0.0007792838150635362}, {"id": 3, "seek": 0, "start": 15.44, "end": 20.32, "text": " Its only real selling point is vibes, and is supposed to chat in a more natural, human-like way.", "tokens": [51136, 6953, 787, 957, 6511, 935, 307, 27636, 11, 293, 307, 3442, 281, 5081, 294, 257, 544, 3303, 11, 1952, 12, 4092, 636, 13, 51380], "temperature": 0.0, "avg_logprob": -0.133174920693422, "compression_ratio": 1.531986531986532, "no_speech_prob": 0.0007792838150635362}, {"id": 4, "seek": 0, "start": 20.32, "end": 24.68, "text": " Don't get me wrong, it's a good model, but not good enough to feed the AI hype monster,", "tokens": [51380, 1468, 380, 483, 385, 2085, 11, 309, 311, 257, 665, 2316, 11, 457, 406, 665, 1547, 281, 3154, 264, 7318, 24144, 10090, 11, 51598], "temperature": 0.0, "avg_logprob": -0.133174920693422, "compression_ratio": 1.531986531986532, "no_speech_prob": 0.0007792838150635362}, {"id": 5, "seek": 2468, "start": 24.68, "end": 30.48, "text": " and it looks increasingly likely that we're not headed into a technological singularity, but rather a sigmoid of sorrow.", "tokens": [50364, 293, 309, 1542, 12980, 3700, 300, 321, 434, 406, 12798, 666, 257, 18439, 20010, 507, 11, 457, 2831, 257, 4556, 3280, 327, 295, 23027, 13, 50654], "temperature": 0.0, "avg_logprob": -0.09057764446034151, "compression_ratio": 1.5870786516853932, "no_speech_prob": 0.32728543877601624}, {"id": 6, "seek": 2468, "start": 30.48, "end": 35.16, "text": " The Sam Altman couldn't even be bothered to leave his newborn kid in the hospital to show up to the product launch,", "tokens": [50654, 440, 4832, 15992, 1601, 2809, 380, 754, 312, 22996, 281, 1856, 702, 32928, 1636, 294, 264, 4530, 281, 855, 493, 281, 264, 1674, 4025, 11, 50888], "temperature": 0.0, "avg_logprob": -0.09057764446034151, "compression_ratio": 1.5870786516853932, "no_speech_prob": 0.32728543877601624}, {"id": 7, "seek": 2468, "start": 35.16, "end": 37.480000000000004, "text": " and instead send in a bunch of interns to demo it.", "tokens": [50888, 293, 2602, 2845, 294, 257, 3840, 295, 46145, 281, 10723, 309, 13, 51004], "temperature": 0.0, "avg_logprob": -0.09057764446034151, "compression_ratio": 1.5870786516853932, "no_speech_prob": 0.32728543877601624}, {"id": 8, "seek": 2468, "start": 37.480000000000004, "end": 40.08, "text": " And that's crazy, because we're talking about Orion here.", "tokens": [51004, 400, 300, 311, 3219, 11, 570, 321, 434, 1417, 466, 41028, 510, 13, 51134], "temperature": 0.0, "avg_logprob": -0.09057764446034151, "compression_ratio": 1.5870786516853932, "no_speech_prob": 0.32728543877601624}, {"id": 9, "seek": 2468, "start": 40.08, "end": 44.120000000000005, "text": " In 2023, tech leaders signed a petition to stop training big models like this.", "tokens": [51134, 682, 44377, 11, 7553, 3523, 8175, 257, 22661, 281, 1590, 3097, 955, 5245, 411, 341, 13, 51336], "temperature": 0.0, "avg_logprob": -0.09057764446034151, "compression_ratio": 1.5870786516853932, "no_speech_prob": 0.32728543877601624}, {"id": 10, "seek": 2468, "start": 44.120000000000005, "end": 50.72, "text": " Altman himself begged the government to regulate it, and the only thing more disappointing than GPT 4.5 is the release of the Epstein files.", "tokens": [51336, 15992, 1601, 3647, 47653, 264, 2463, 281, 24475, 309, 11, 293, 264, 787, 551, 544, 25054, 813, 26039, 51, 1017, 13, 20, 307, 264, 4374, 295, 264, 9970, 9089, 7098, 13, 51666], "temperature": 0.0, "avg_logprob": -0.09057764446034151, "compression_ratio": 1.5870786516853932, "no_speech_prob": 0.32728543877601624}, {"id": 11, "seek": 5072, "start": 50.76, "end": 56.2, "text": " In today's video, we'll find out if we just reached the limits of pre-training and generative pre-trained Transformers.", "tokens": [50366, 682, 965, 311, 960, 11, 321, 603, 915, 484, 498, 321, 445, 6488, 264, 10406, 295, 659, 12, 17227, 1760, 293, 1337, 1166, 659, 12, 17227, 2001, 27938, 433, 13, 50638], "temperature": 0.0, "avg_logprob": -0.08787612497371479, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.17756257951259613}, {"id": 12, "seek": 5072, "start": 56.2, "end": 60.04, "text": " It is February 28th, 2025, and you're watching the Code Report.", "tokens": [50638, 467, 307, 8711, 7562, 392, 11, 39209, 11, 293, 291, 434, 1976, 264, 15549, 16057, 13, 50830], "temperature": 0.0, "avg_logprob": -0.08787612497371479, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.17756257951259613}, {"id": 13, "seek": 5072, "start": 60.04, "end": 63.84, "text": " I didn't want to make another crappy AI video today, but the bat signal was triggered.", "tokens": [50830, 286, 994, 380, 528, 281, 652, 1071, 36531, 7318, 960, 965, 11, 457, 264, 7362, 6358, 390, 21710, 13, 51020], "temperature": 0.0, "avg_logprob": -0.08787612497371479, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.17756257951259613}, {"id": 14, "seek": 5072, "start": 63.84, "end": 67.96000000000001, "text": " Any time an official video gets ratioed like this, I have no choice but to make a video.", "tokens": [51020, 2639, 565, 364, 4783, 960, 2170, 8509, 292, 411, 341, 11, 286, 362, 572, 3922, 457, 281, 652, 257, 960, 13, 51226], "temperature": 0.0, "avg_logprob": -0.08787612497371479, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.17756257951259613}, {"id": 15, "seek": 5072, "start": 67.96000000000001, "end": 71.44, "text": " Before you unsubscribe, though, I've got an interesting Postgres video on the way.", "tokens": [51226, 4546, 291, 2693, 9493, 11, 1673, 11, 286, 600, 658, 364, 1880, 10223, 45189, 960, 322, 264, 636, 13, 51400], "temperature": 0.0, "avg_logprob": -0.08787612497371479, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.17756257951259613}, {"id": 16, "seek": 5072, "start": 71.44, "end": 75.24, "text": " The first thing to know about GPT 4.5 is that it's extremely expensive.", "tokens": [51400, 440, 700, 551, 281, 458, 466, 26039, 51, 1017, 13, 20, 307, 300, 309, 311, 4664, 5124, 13, 51590], "temperature": 0.0, "avg_logprob": -0.08787612497371479, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.17756257951259613}, {"id": 17, "seek": 7524, "start": 75.24, "end": 78.47999999999999, "text": " If you thought Claude was expensive at $15 per million tokens,", "tokens": [50364, 759, 291, 1194, 12947, 2303, 390, 5124, 412, 1848, 5211, 680, 2459, 22667, 11, 50526], "temperature": 0.0, "avg_logprob": -0.09768363611022038, "compression_ratio": 1.6688102893890675, "no_speech_prob": 0.080295629799366}, {"id": 18, "seek": 7524, "start": 78.47999999999999, "end": 83.8, "text": " GPT 4.5 is five times more expensive at $75 per million output tokens.", "tokens": [50526, 26039, 51, 1017, 13, 20, 307, 1732, 1413, 544, 5124, 412, 1848, 11901, 680, 2459, 5598, 22667, 13, 50792], "temperature": 0.0, "avg_logprob": -0.09768363611022038, "compression_ratio": 1.6688102893890675, "no_speech_prob": 0.080295629799366}, {"id": 19, "seek": 7524, "start": 83.8, "end": 86.28, "text": " Actually, no, correction, that's input tokens.", "tokens": [50792, 5135, 11, 572, 11, 19984, 11, 300, 311, 4846, 22667, 13, 50916], "temperature": 0.0, "avg_logprob": -0.09768363611022038, "compression_ratio": 1.6688102893890675, "no_speech_prob": 0.080295629799366}, {"id": 20, "seek": 7524, "start": 86.28, "end": 88.96, "text": " It's $150 per million output tokens.", "tokens": [50916, 467, 311, 1848, 20120, 680, 2459, 5598, 22667, 13, 51050], "temperature": 0.0, "avg_logprob": -0.09768363611022038, "compression_ratio": 1.6688102893890675, "no_speech_prob": 0.080295629799366}, {"id": 21, "seek": 7524, "start": 88.96, "end": 93.16, "text": " And to chat with it, it's currently only available to the $200 per month pro users.", "tokens": [51050, 400, 281, 5081, 365, 309, 11, 309, 311, 4362, 787, 2435, 281, 264, 1848, 7629, 680, 1618, 447, 5022, 13, 51260], "temperature": 0.0, "avg_logprob": -0.09768363611022038, "compression_ratio": 1.6688102893890675, "no_speech_prob": 0.080295629799366}, {"id": 22, "seek": 7524, "start": 93.16, "end": 98.32, "text": " I tried it out myself, and it does seem to emit chill vibes, but the problem is that's highly subjective.", "tokens": [51260, 286, 3031, 309, 484, 2059, 11, 293, 309, 775, 1643, 281, 32084, 11355, 27636, 11, 457, 264, 1154, 307, 300, 311, 5405, 25972, 13, 51518], "temperature": 0.0, "avg_logprob": -0.09768363611022038, "compression_ratio": 1.6688102893890675, "no_speech_prob": 0.080295629799366}, {"id": 23, "seek": 7524, "start": 98.32, "end": 103.8, "text": " However, in the launch, OpenAI talked about a new vibes benchmark that's supposed to measure creative thinking.", "tokens": [51518, 2908, 11, 294, 264, 4025, 11, 7238, 48698, 2825, 466, 257, 777, 27636, 18927, 300, 311, 3442, 281, 3481, 5880, 1953, 13, 51792], "temperature": 0.0, "avg_logprob": -0.09768363611022038, "compression_ratio": 1.6688102893890675, "no_speech_prob": 0.080295629799366}, {"id": 24, "seek": 10380, "start": 103.8, "end": 107.6, "text": " The best way to get a feel for the model is to talk to it, so let's jump into a demo.", "tokens": [50364, 440, 1151, 636, 281, 483, 257, 841, 337, 264, 2316, 307, 281, 751, 281, 309, 11, 370, 718, 311, 3012, 666, 257, 10723, 13, 50554], "temperature": 0.0, "avg_logprob": -0.08882715437147352, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.017437275499105453}, {"id": 25, "seek": 10380, "start": 107.6, "end": 112.67999999999999, "text": " A lot of people on the internet criticized this presentation, but as an introvert myself, I think they did a great job.", "tokens": [50554, 316, 688, 295, 561, 322, 264, 4705, 28011, 341, 5860, 11, 457, 382, 364, 12897, 3281, 2059, 11, 286, 519, 436, 630, 257, 869, 1691, 13, 50808], "temperature": 0.0, "avg_logprob": -0.08882715437147352, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.017437275499105453}, {"id": 26, "seek": 10380, "start": 112.67999999999999, "end": 118.56, "text": " In addition, it apparently has a far lower hallucination rate, but what I found is that it still makes a lot of silly mistakes.", "tokens": [50808, 682, 4500, 11, 309, 7970, 575, 257, 1400, 3126, 35212, 2486, 3314, 11, 457, 437, 286, 1352, 307, 300, 309, 920, 1669, 257, 688, 295, 11774, 8038, 13, 51102], "temperature": 0.0, "avg_logprob": -0.08882715437147352, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.017437275499105453}, {"id": 27, "seek": 10380, "start": 118.56, "end": 125.2, "text": " It's not self-aware and has no idea what GPT 4.5 even is, and says its training cutoff is October 2023.", "tokens": [51102, 467, 311, 406, 2698, 12, 17074, 293, 575, 572, 1558, 437, 26039, 51, 1017, 13, 20, 754, 307, 11, 293, 1619, 1080, 3097, 1723, 4506, 307, 7617, 44377, 13, 51434], "temperature": 0.0, "avg_logprob": -0.08882715437147352, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.017437275499105453}, {"id": 28, "seek": 10380, "start": 125.2, "end": 128.4, "text": " It was, however, able to tell me how many Rs are in strawberry.", "tokens": [51434, 467, 390, 11, 4461, 11, 1075, 281, 980, 385, 577, 867, 21643, 366, 294, 20440, 13, 51594], "temperature": 0.0, "avg_logprob": -0.08882715437147352, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.017437275499105453}, {"id": 29, "seek": 12840, "start": 128.4, "end": 134.08, "text": " That felt like a huge leap forward, but I quickly became disappointed when it gave me the wrong number of Ls in Lollapalooza.", "tokens": [50364, 663, 2762, 411, 257, 2603, 19438, 2128, 11, 457, 286, 2661, 3062, 13856, 562, 309, 2729, 385, 264, 2085, 1230, 295, 441, 82, 294, 441, 1833, 569, 304, 1986, 2394, 13, 50648], "temperature": 0.0, "avg_logprob": -0.10883036325144213, "compression_ratio": 1.6532663316582914, "no_speech_prob": 0.18696776032447815}, {"id": 30, "seek": 12840, "start": 134.08, "end": 140.48000000000002, "text": " Now, when it comes to programming and science, I didn't even try, because we already know it's not going to perform as well as the deep thinking models like 03.", "tokens": [50648, 823, 11, 562, 309, 1487, 281, 9410, 293, 3497, 11, 286, 994, 380, 754, 853, 11, 570, 321, 1217, 458, 309, 311, 406, 516, 281, 2042, 382, 731, 382, 264, 2452, 1953, 5245, 411, 43677, 13, 50968], "temperature": 0.0, "avg_logprob": -0.10883036325144213, "compression_ratio": 1.6532663316582914, "no_speech_prob": 0.18696776032447815}, {"id": 31, "seek": 12840, "start": 140.48000000000002, "end": 147.92000000000002, "text": " Then to make matters worse, on the AIDAR polyglot coding benchmark, it's not only worse at programming than DeepSeek, but also hundreds of times more expensive.", "tokens": [50968, 1396, 281, 652, 7001, 5324, 11, 322, 264, 316, 2777, 1899, 6754, 7191, 310, 17720, 18927, 11, 309, 311, 406, 787, 5324, 412, 9410, 813, 14895, 10637, 916, 11, 457, 611, 6779, 295, 1413, 544, 5124, 13, 51340], "temperature": 0.0, "avg_logprob": -0.10883036325144213, "compression_ratio": 1.6532663316582914, "no_speech_prob": 0.18696776032447815}, {"id": 32, "seek": 12840, "start": 147.92000000000002, "end": 154.8, "text": " Now, if you're an Elon Musk hater, you'll want to take a bong rip of Copium right now, because currently XAI's Grok is the best model in the world.", "tokens": [51340, 823, 11, 498, 291, 434, 364, 28498, 26019, 276, 771, 11, 291, 603, 528, 281, 747, 257, 272, 556, 12782, 295, 11579, 2197, 558, 586, 11, 570, 4362, 1783, 48698, 311, 12981, 74, 307, 264, 1151, 2316, 294, 264, 1002, 13, 51684], "temperature": 0.0, "avg_logprob": -0.10883036325144213, "compression_ratio": 1.6532663316582914, "no_speech_prob": 0.18696776032447815}, {"id": 33, "seek": 12840, "start": 154.8, "end": 157.44, "text": " That's not my opinion, it's the opinion of the betting market.", "tokens": [51684, 663, 311, 406, 452, 4800, 11, 309, 311, 264, 4800, 295, 264, 34246, 2142, 13, 51816], "temperature": 0.0, "avg_logprob": -0.10883036325144213, "compression_ratio": 1.6532663316582914, "no_speech_prob": 0.18696776032447815}, {"id": 34, "seek": 15744, "start": 157.44, "end": 162.96, "text": " Although by the end of 2025, OpenAI is still the favorite to have the best model, but its odds are on the decline.", "tokens": [50364, 5780, 538, 264, 917, 295, 39209, 11, 7238, 48698, 307, 920, 264, 2954, 281, 362, 264, 1151, 2316, 11, 457, 1080, 17439, 366, 322, 264, 15635, 13, 50640], "temperature": 0.0, "avg_logprob": -0.10005418843236463, "compression_ratio": 1.6779220779220778, "no_speech_prob": 0.026747403666377068}, {"id": 35, "seek": 15744, "start": 162.96, "end": 170.84, "text": " That's problematic for OpenAI though, because they're raising billions and billions of dollars as they transition to for-profit and will need to maintain a massive valuation.", "tokens": [50640, 663, 311, 19011, 337, 7238, 48698, 1673, 11, 570, 436, 434, 11225, 17375, 293, 17375, 295, 3808, 382, 436, 6034, 281, 337, 12, 14583, 293, 486, 643, 281, 6909, 257, 5994, 38546, 13, 51034], "temperature": 0.0, "avg_logprob": -0.10005418843236463, "compression_ratio": 1.6779220779220778, "no_speech_prob": 0.026747403666377068}, {"id": 36, "seek": 15744, "start": 170.84, "end": 175.0, "text": " Altman says there is no wall and believes they can scale these models almost infinitely.", "tokens": [51034, 15992, 1601, 1619, 456, 307, 572, 2929, 293, 12307, 436, 393, 4373, 613, 5245, 1920, 36227, 13, 51242], "temperature": 0.0, "avg_logprob": -0.10005418843236463, "compression_ratio": 1.6779220779220778, "no_speech_prob": 0.026747403666377068}, {"id": 37, "seek": 15744, "start": 175.0, "end": 179.28, "text": " That's assuming he gets trillions of dollars from SoftBank and the Saudis to build these data centers.", "tokens": [51242, 663, 311, 11926, 415, 2170, 504, 46279, 295, 3808, 490, 16985, 33, 657, 293, 264, 15717, 271, 281, 1322, 613, 1412, 10898, 13, 51456], "temperature": 0.0, "avg_logprob": -0.10005418843236463, "compression_ratio": 1.6779220779220778, "no_speech_prob": 0.026747403666377068}, {"id": 38, "seek": 15744, "start": 179.28, "end": 187.12, "text": " My theory as an unqualified shipposter is that they failed to train GPT 5 with any significant improvement, despite scaling up the number of parameters and compute.", "tokens": [51456, 1222, 5261, 382, 364, 517, 46094, 402, 2488, 7096, 307, 300, 436, 7612, 281, 3847, 26039, 51, 1025, 365, 604, 4776, 10444, 11, 7228, 21589, 493, 264, 1230, 295, 9834, 293, 14722, 13, 51848], "temperature": 0.0, "avg_logprob": -0.10005418843236463, "compression_ratio": 1.6779220779220778, "no_speech_prob": 0.026747403666377068}, {"id": 39, "seek": 18712, "start": 187.12, "end": 197.64000000000001, "text": " GPT 4.5 is the biggest model they've ever created, and now they're lowering the bar for GPT 5, which Altman described a few weeks ago being more like a router that automatically chooses the best model based on your prompt.", "tokens": [50364, 26039, 51, 1017, 13, 20, 307, 264, 3880, 2316, 436, 600, 1562, 2942, 11, 293, 586, 436, 434, 28124, 264, 2159, 337, 26039, 51, 1025, 11, 597, 15992, 1601, 7619, 257, 1326, 3259, 2057, 885, 544, 411, 257, 22492, 300, 6772, 25963, 264, 1151, 2316, 2361, 322, 428, 12391, 13, 50890], "temperature": 0.0, "avg_logprob": -0.10525097269000429, "compression_ratio": 1.625, "no_speech_prob": 0.007814230397343636}, {"id": 40, "seek": 18712, "start": 197.64000000000001, "end": 205.6, "text": " And that's highly disappointing, because I was expecting to be a post-apocalyptic warlord by now, battling robots and barbecuing rats over burning garbage cans for dinner.", "tokens": [50890, 400, 300, 311, 5405, 25054, 11, 570, 286, 390, 9650, 281, 312, 257, 2183, 12, 569, 47407, 1516, 22839, 538, 586, 11, 33752, 14733, 293, 2159, 8123, 9635, 25691, 670, 9488, 14150, 21835, 337, 6148, 13, 51288], "temperature": 0.0, "avg_logprob": -0.10525097269000429, "compression_ratio": 1.625, "no_speech_prob": 0.007814230397343636}, {"id": 41, "seek": 18712, "start": 205.6, "end": 210.76, "text": " But instead I live in this dystopia where artificial superintelligence never comes and nothing ever happens.", "tokens": [51288, 583, 2602, 286, 1621, 294, 341, 14584, 13559, 654, 689, 11677, 1687, 20761, 17644, 1128, 1487, 293, 1825, 1562, 2314, 13, 51546], "temperature": 0.0, "avg_logprob": -0.10525097269000429, "compression_ratio": 1.625, "no_speech_prob": 0.007814230397343636}, {"id": 42, "seek": 18712, "start": 210.76, "end": 213.84, "text": " But if you're a computer science student, the plateau is great news.", "tokens": [51546, 583, 498, 291, 434, 257, 3820, 3497, 3107, 11, 264, 39885, 307, 869, 2583, 13, 51700], "temperature": 0.0, "avg_logprob": -0.10525097269000429, "compression_ratio": 1.625, "no_speech_prob": 0.007814230397343636}, {"id": 43, "seek": 21384, "start": 213.88, "end": 218.52, "text": " AI coding tools are incredible, but they're most useful to real human programmers who know what they're doing.", "tokens": [50366, 7318, 17720, 3873, 366, 4651, 11, 457, 436, 434, 881, 4420, 281, 957, 1952, 41504, 567, 458, 437, 436, 434, 884, 13, 50598], "temperature": 0.0, "avg_logprob": -0.10281180435756468, "compression_ratio": 1.627062706270627, "no_speech_prob": 0.21180376410484314}, {"id": 44, "seek": 21384, "start": 218.52, "end": 220.48000000000002, "text": " And I don't see that changing anytime soon.", "tokens": [50598, 400, 286, 500, 380, 536, 300, 4473, 13038, 2321, 13, 50696], "temperature": 0.0, "avg_logprob": -0.10281180435756468, "compression_ratio": 1.627062706270627, "no_speech_prob": 0.21180376410484314}, {"id": 45, "seek": 21384, "start": 220.48000000000002, "end": 224.68, "text": " And you can start getting really good at programming for free thanks to this video's sponsor, Brilliant.", "tokens": [50696, 400, 291, 393, 722, 1242, 534, 665, 412, 9410, 337, 1737, 3231, 281, 341, 960, 311, 16198, 11, 34007, 13, 50906], "temperature": 0.0, "avg_logprob": -0.10281180435756468, "compression_ratio": 1.627062706270627, "no_speech_prob": 0.21180376410484314}, {"id": 46, "seek": 21384, "start": 224.68, "end": 230.28, "text": " Their platform provides interactive hands-on lessons that demystify the complexity of deep learning.", "tokens": [50906, 6710, 3663, 6417, 15141, 2377, 12, 266, 8820, 300, 1371, 38593, 2505, 264, 14024, 295, 2452, 2539, 13, 51186], "temperature": 0.0, "avg_logprob": -0.10281180435756468, "compression_ratio": 1.627062706270627, "no_speech_prob": 0.21180376410484314}, {"id": 47, "seek": 21384, "start": 230.28, "end": 236.96, "text": " With just a few minutes of effort each day, you can understand the math and computer science behind this seemingly magic technology.", "tokens": [51186, 2022, 445, 257, 1326, 2077, 295, 4630, 1184, 786, 11, 291, 393, 1223, 264, 5221, 293, 3820, 3497, 2261, 341, 18709, 5585, 2899, 13, 51520], "temperature": 0.0, "avg_logprob": -0.10281180435756468, "compression_ratio": 1.627062706270627, "no_speech_prob": 0.21180376410484314}, {"id": 48, "seek": 23696, "start": 237.0, "end": 244.32000000000002, "text": " I'd recommend starting with Python, then check out their full How Large Language Models Work course if you really want to look under the hood of chat GPT.", "tokens": [50366, 286, 1116, 2748, 2891, 365, 15329, 11, 550, 1520, 484, 641, 1577, 1012, 33092, 24445, 6583, 1625, 6603, 1164, 498, 291, 534, 528, 281, 574, 833, 264, 13376, 295, 5081, 26039, 51, 13, 50732], "temperature": 0.0, "avg_logprob": -0.12235127138287834, "compression_ratio": 1.4703557312252964, "no_speech_prob": 0.18895600736141205}, {"id": 49, "seek": 23696, "start": 244.32000000000002, "end": 251.96, "text": " Try everything Brilliant has to offer for free for 30 days by going to brilliant.org slash fireship or use the QR code on screen.", "tokens": [50732, 6526, 1203, 34007, 575, 281, 2626, 337, 1737, 337, 2217, 1708, 538, 516, 281, 10248, 13, 4646, 17330, 15044, 1210, 420, 764, 264, 32784, 3089, 322, 2568, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12235127138287834, "compression_ratio": 1.4703557312252964, "no_speech_prob": 0.18895600736141205}, {"id": 50, "seek": 23696, "start": 251.96, "end": 255.52, "text": " This has been the Code Report. Thanks for watching, and I will see you in the next one.", "tokens": [51114, 639, 575, 668, 264, 15549, 16057, 13, 2561, 337, 1976, 11, 293, 286, 486, 536, 291, 294, 264, 958, 472, 13, 51292], "temperature": 0.0, "avg_logprob": -0.12235127138287834, "compression_ratio": 1.4703557312252964, "no_speech_prob": 0.18895600736141205}], "title": "GPT-4.5 shocks the world with its lack of intelligence.", "channel": "fireship"}